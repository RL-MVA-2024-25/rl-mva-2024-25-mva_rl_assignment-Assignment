{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "from env_hiv import HIVPatient\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from replay_buffer import ReplayBuffer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from DQN import DQN\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TimeLimit(\n",
    "    env=HIVPatient(domain_randomization=False), max_episode_steps=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(network, state):\n",
    "    device = next(network.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n",
    "        return torch.argmax(Q).item()\n",
    "\n",
    "class ProjectAgent:\n",
    "    def __init__(self):\n",
    "        self.n_actions = 4\n",
    "        self.state_dim = 6\n",
    "        self.gamma = 0.85 \n",
    "        self.device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.save_path = \"agent.pt\"\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=60000,device = self.device)\n",
    "        self.model = DQN(self.state_dim, self.n_actions).to(self.device)\n",
    "        self.lr = 1e-3 \n",
    "        self.batch_size = 1024\n",
    "\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_min =  0.01\n",
    "        self.epsilon_stop = 1000\n",
    "        self.epsilon_delay = 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "\n",
    "        \n",
    "        self.update_count = 0\n",
    "        self.target_update_freq = 100\n",
    "        self.update_target_tau = 0.005\n",
    "        self.update_target_strategy = 'ema'\n",
    "        self.nb_gradient_steps = 1\n",
    " \n",
    "        #self.q_network = QNetwork(self.state_dim, self.n_actions).to(self.device)\n",
    "        self.target_network = DQN(self.state_dim, self.n_actions).to(self.device)\n",
    "           \n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        self.monitoring_nb_trials = 40\n",
    "        self.monitor_every =  40\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),lr=self.lr)                     \n",
    "        #self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=350, gamma=0.5)\n",
    "    \n",
    "    def MC_eval(self, env, nb_trials):   \n",
    "        MC_total_reward = []\n",
    "        MC_discounted_reward = []\n",
    "        for _ in range(nb_trials):\n",
    "            x,_ = env.reset()\n",
    "            done = False\n",
    "            trunc = False\n",
    "            total_reward = 0\n",
    "            discounted_reward = 0\n",
    "            step = 0\n",
    "            while not (done or trunc):\n",
    "                a = greedy_action(self.model, x)\n",
    "                y,r,done,trunc,_ = env.step(a)\n",
    "                x = y\n",
    "                total_reward += r\n",
    "                discounted_reward += self.gamma**step * r\n",
    "                step += 1\n",
    "            MC_total_reward.append(total_reward)\n",
    "            MC_discounted_reward.append(discounted_reward)\n",
    "        return np.mean(MC_discounted_reward), np.mean(MC_total_reward)\n",
    "\n",
    "    def V_initial_state(self, env, nb_trials):   \n",
    "        with torch.no_grad():\n",
    "            for _ in range(nb_trials):\n",
    "                val = []\n",
    "                x,_ = env.reset()\n",
    "                val.append(self.model(torch.Tensor(x).unsqueeze(0).to(self.device)).max().item())\n",
    "        return np.mean(val)\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.replay_buffer) > self.batch_size:\n",
    "            X, A, R, Y, D = self.replay_buffer.sample(self.batch_size)\n",
    "            QYmax = self.target_network(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "\n",
    "    def act(self, observation, use_random=False):\n",
    "        if use_random:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return greedy_action(self.model, observation)\n",
    "\n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        MC_avg_total_reward = []   # NEW NEW NEW\n",
    "        MC_avg_discounted_reward = []   # NEW NEW NEW\n",
    "        V_init_state = []   # NEW NEW NEW\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "            # select epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(self.model, state)\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.replay_buffer.append(state, action, reward, next_state, done)\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_network.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_network.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                self.target_network.load_state_dict(target_state_dict)\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done or trunc:\n",
    "                episode += 1\n",
    "                # Monitoring\n",
    "                if self.monitoring_nb_trials>0:\n",
    "                    MC_dr, MC_tr = self.MC_eval(env, self.monitoring_nb_trials)    # NEW NEW NEW\n",
    "                    V0 = self.V_initial_state(env, self.monitoring_nb_trials)   # NEW NEW NEW\n",
    "                    MC_avg_total_reward.append(MC_tr)   # NEW NEW NEW\n",
    "                    MC_avg_discounted_reward.append(MC_dr)   # NEW NEW NEW\n",
    "                    V_init_state.append(V0)   # NEW NEW NEW\n",
    "                    episode_return.append(episode_cum_reward)   # NEW NEW NEW\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.replay_buffer)), \n",
    "                          \", ep return \", '{:4.1f}'.format(episode_cum_reward), \n",
    "                          \", MC tot \", '{:6.2f}'.format(MC_tr),\n",
    "                          \", MC disc \", '{:6.2f}'.format(MC_dr),\n",
    "                          \", V0 \", '{:6.2f}'.format(V0),\n",
    "                          sep='')\n",
    "                else:\n",
    "                    episode_return.append(episode_cum_reward)\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.replay_buffer)), \n",
    "                          \", ep return \", '{:4.1f}'.format(episode_cum_reward), \n",
    "                          sep='')\n",
    "\n",
    "                \n",
    "                state, _ = env.reset()\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "        return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f\"Model saved in {path}\")\n",
    "\n",
    "    def load(self):\n",
    "        self.model.load_state_dict(torch.load(self.save_path, map_location=self.device))\n",
    "        self.target_network = deepcopy(self.model).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def collect_sample(self,nb_sample):\n",
    "        s, _ = env.reset()\n",
    "        for _ in range(nb_sample):\n",
    "            a = self.act(s)\n",
    "            s2, r, done, trunc, _ = env.step(a)\n",
    "            self.replay_buffer.append(s, a, r, s2, done)\n",
    "            if done or trunc :\n",
    "                s, _ = env.reset()\n",
    "            else:\n",
    "                s = s2\n",
    "        print('end of collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Agent created\n",
      "end of collection\n",
      "Episode  1, epsilon   0.82, batch size 1200, ep return 9718711.5, MC tot 2767769.38, MC disc 121578.42, V0 31670.08\n",
      "Episode  2, epsilon   0.62, batch size 1400, ep return 11394152.3, MC tot 6787976.82, MC disc 81821.88, V0 52685.08\n",
      "Episode  3, epsilon   0.43, batch size 1600, ep return 7664525.7, MC tot 4068046.30, MC disc 181834.56, V0 63737.53\n",
      "Episode  4, epsilon   0.23, batch size 1800, ep return 10174233.5, MC tot 6787976.82, MC disc 81821.88, V0 72945.48\n",
      "Episode  5, epsilon   0.03, batch size 2000, ep return 15983697.0, MC tot 12904030.71, MC disc 297553.38, V0 87885.02\n",
      "Episode  6, epsilon   0.01, batch size 2200, ep return 28171138.5, MC tot 6787976.82, MC disc 81821.88, V0 108611.08\n",
      "Episode  7, epsilon   0.01, batch size 2400, ep return 7167058.9, MC tot 6787976.82, MC disc 81821.88, V0 118914.97\n",
      "Episode  8, epsilon   0.01, batch size 2600, ep return 7036090.3, MC tot 6787976.82, MC disc 81821.88, V0 142226.66\n",
      "Episode  9, epsilon   0.01, batch size 2800, ep return 7177934.5, MC tot 3567474.06, MC disc 79461.49, V0 141446.94\n",
      "Episode 10, epsilon   0.01, batch size 3000, ep return 6676846.0, MC tot 11731822.59, MC disc 107513.91, V0 155292.44\n",
      "Episode 11, epsilon   0.01, batch size 3200, ep return 12532430.6, MC tot 11337262.40, MC disc 115730.13, V0 144472.23\n",
      "Episode 12, epsilon   0.01, batch size 3400, ep return 13133819.8, MC tot 8951549.30, MC disc 129804.63, V0 151712.77\n",
      "Episode 13, epsilon   0.01, batch size 3600, ep return 14921628.3, MC tot 8581975.80, MC disc 125085.76, V0 137397.25\n",
      "Episode 14, epsilon   0.01, batch size 3800, ep return 20803570.9, MC tot 14093105.76, MC disc 129947.19, V0 132248.67\n",
      "Episode 15, epsilon   0.01, batch size 4000, ep return 9404863.3, MC tot 10230656.88, MC disc 122052.38, V0 133048.53\n",
      "Episode 16, epsilon   0.01, batch size 4200, ep return 10230656.9, MC tot 9767327.50, MC disc 115415.77, V0 125987.69\n",
      "Episode 17, epsilon   0.01, batch size 4400, ep return 11425512.2, MC tot 10230656.88, MC disc 122052.38, V0 134175.25\n",
      "Episode 18, epsilon   0.01, batch size 4600, ep return 10588362.3, MC tot 11296087.24, MC disc 139854.39, V0 121311.11\n",
      "Episode 19, epsilon   0.01, batch size 4800, ep return 23139047.5, MC tot 10512149.15, MC disc 164306.03, V0 121987.71\n",
      "Episode 20, epsilon   0.01, batch size 5000, ep return 20582085.8, MC tot 12722443.22, MC disc 160937.54, V0 112812.02\n",
      "Episode 21, epsilon   0.01, batch size 5200, ep return 18975633.6, MC tot 12309120.43, MC disc 159863.89, V0 118151.20\n",
      "Episode 22, epsilon   0.01, batch size 5400, ep return 20348446.8, MC tot 23598816.31, MC disc 159864.84, V0 104970.87\n",
      "Episode 23, epsilon   0.01, batch size 5600, ep return 15560247.8, MC tot 27830862.62, MC disc 157781.25, V0 107236.55\n",
      "Episode 24, epsilon   0.01, batch size 5800, ep return 26139279.2, MC tot 23888692.10, MC disc 159865.68, V0 90296.09\n",
      "Episode 25, epsilon   0.01, batch size 6000, ep return 31885231.2, MC tot 11934344.53, MC disc 145279.63, V0 96142.30\n",
      "Episode 26, epsilon   0.01, batch size 6200, ep return 23742945.6, MC tot 11934344.53, MC disc 145279.63, V0 110292.30\n",
      "Episode 27, epsilon   0.01, batch size 6400, ep return 26525385.9, MC tot 11934344.53, MC disc 145279.63, V0 95180.67\n",
      "Episode 28, epsilon   0.01, batch size 6600, ep return 27263007.3, MC tot 6787976.82, MC disc 81821.88, V0 109493.02\n",
      "Episode 29, epsilon   0.01, batch size 6800, ep return 15177549.7, MC tot 11242956.38, MC disc 138009.77, V0 93180.18\n",
      "Episode 30, epsilon   0.01, batch size 7000, ep return 25415269.0, MC tot 29633571.85, MC disc 157779.71, V0 103439.02\n",
      "Episode 31, epsilon   0.01, batch size 7200, ep return 26051168.9, MC tot 28954672.63, MC disc 147896.04, V0 106594.26\n",
      "Episode 32, epsilon   0.01, batch size 7400, ep return 26142806.1, MC tot 24176701.42, MC disc 160934.83, V0 99069.62\n",
      "Episode 33, epsilon   0.01, batch size 7600, ep return 24626543.7, MC tot 28943821.70, MC disc 147896.18, V0 119721.94\n",
      "Episode 34, epsilon   0.01, batch size 7800, ep return 29122646.0, MC tot 11242956.38, MC disc 138009.77, V0 130759.53\n",
      "Episode 35, epsilon   0.01, batch size 8000, ep return 28784616.0, MC tot 28885753.23, MC disc 157779.57, V0 145549.02\n",
      "Episode 36, epsilon   0.01, batch size 8200, ep return 24062097.1, MC tot 28171602.63, MC disc 147895.71, V0 114723.16\n",
      "Episode 37, epsilon   0.01, batch size 8400, ep return 28909314.2, MC tot 28954672.63, MC disc 147896.04, V0 135477.48\n",
      "Episode 38, epsilon   0.01, batch size 8600, ep return 26750202.8, MC tot 24789582.47, MC disc 138009.50, V0 134008.12\n",
      "Episode 39, epsilon   0.01, batch size 8800, ep return 24337580.6, MC tot 3432807.68, MC disc 115806.34, V0 123948.86\n",
      "Episode 40, epsilon   0.01, batch size 9000, ep return 26452959.5, MC tot 9475265.49, MC disc 81740.37, V0 76403.50\n",
      "Episode 41, epsilon   0.01, batch size 9200, ep return 20750651.1, MC tot 19162709.66, MC disc 102557.50, V0 119572.86\n",
      "Episode 42, epsilon   0.01, batch size 9400, ep return 22499775.4, MC tot 19782807.83, MC disc 102557.45, V0 128615.85\n",
      "Episode 43, epsilon   0.01, batch size 9600, ep return 23261127.0, MC tot 30848620.09, MC disc 125335.03, V0 114052.16\n",
      "Episode 44, epsilon   0.01, batch size 9800, ep return 23126240.7, MC tot 7674266.81, MC disc 91022.75, V0 148899.95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgent created\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39mcollect_sample(\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(env, \u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 112\u001b[0m, in \u001b[0;36mProjectAgent.train\u001b[0;34m(self, env, max_episode)\u001b[0m\n\u001b[1;32m    110\u001b[0m     action \u001b[38;5;241m=\u001b[39m greedy_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, state)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# step\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m next_state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mappend(state, action, reward, next_state, done)\n\u001b[1;32m    114\u001b[0m episode_cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/MVA/mva-rl-assignment-antoine-martinez/src/env_hiv.py:231\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[0;34m(self, a_index)\u001b[0m\n\u001b[1;32m    229\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[1;32m    230\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[0;32m--> 231\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition(state, action, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    232\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[0;32m~/MVA/mva-rl-assignment-antoine-martinez/src/env_hiv.py:212\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[0;34m(self, state, action, duration)\u001b[0m\n\u001b[1;32m    210\u001b[0m nb_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[0;32m--> 212\u001b[0m     der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mder(state0, action)\n\u001b[1;32m    213\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# np.clip(state1, self.lower, self.upper, out=state1)\u001b[39;00m\n",
      "File \u001b[0;32m~/MVA/mva-rl-assignment-antoine-martinez/src/env_hiv.py:167\u001b[0m, in \u001b[0;36mHIVPatient.der\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_patient_parameters()\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate(), {}\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mder\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[1;32m    168\u001b[0m     T1 \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    169\u001b[0m     T1star \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "print('Start')\n",
    "model = DQN(6, 4)\n",
    "agent = ProjectAgent(env, model)\n",
    "print('Agent created')\n",
    "agent.collect_sample(1000)\n",
    "agent.train(env, 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
